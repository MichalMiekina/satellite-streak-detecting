{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data, as it is not a valid image.\n",
      "Skipping file dictionary_0.csv, as it is not a valid image.\n",
      "Skipping file gan_output1, as it is not a valid image.\n",
      "Skipping file gan_output2, as it is not a valid image.\n",
      "Skipping file gan_output3, as it is not a valid image.\n",
      "Skipping file gan_output4, as it is not a valid image.\n",
      "Loaded 220 valid images, skipped 6 invalid images.\n",
      "Dataset shape: (220, 64, 64, 3)\n",
      "1/1 [==============================] - 0s 487ms/step\n",
      "0 [D loss: 0.7043434679508209, acc.: 12.5] [G loss: 0.596942663192749]\n",
      "1/1 [==============================] - 0s 483ms/step\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "1 [D loss: 0.6147266626358032, acc.: 50.0] [G loss: 0.491123765707016]\n",
      "1/1 [==============================] - 0s 305ms/step\n",
      "2 [D loss: 0.5414106249809265, acc.: 59.375] [G loss: 0.37029314041137695]\n",
      "1/1 [==============================] - 0s 327ms/step\n",
      "3 [D loss: 0.42481327056884766, acc.: 100.0] [G loss: 0.27193164825439453]\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "4 [D loss: 0.22690019011497498, acc.: 100.0] [G loss: 0.16041582822799683]\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "5 [D loss: 0.05858895555138588, acc.: 100.0] [G loss: 0.06942424178123474]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "6 [D loss: 0.0147747618611902, acc.: 100.0] [G loss: 0.022701026871800423]\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "7 [D loss: 0.006383502099197358, acc.: 100.0] [G loss: 0.008555682376027107]\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "8 [D loss: 0.003996697691036388, acc.: 100.0] [G loss: 0.0041863019578158855]\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "9 [D loss: 0.0026880394434556365, acc.: 100.0] [G loss: 0.0024689598940312862]\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "10 [D loss: 0.001942615010193549, acc.: 100.0] [G loss: 0.0017263260670006275]\n",
      "1/1 [==============================] - 0s 320ms/step\n",
      "11 [D loss: 0.0016737174155423418, acc.: 100.0] [G loss: 0.0011897848453372717]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "12 [D loss: 0.001367977776681073, acc.: 100.0] [G loss: 0.000942866550758481]\n",
      "1/1 [==============================] - 0s 262ms/step\n",
      "13 [D loss: 0.0011536458478076383, acc.: 100.0] [G loss: 0.000719048548489809]\n",
      "1/1 [==============================] - 0s 297ms/step\n",
      "14 [D loss: 0.0010409090464236215, acc.: 100.0] [G loss: 0.0006018758285790682]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "15 [D loss: 0.0009280539816245437, acc.: 100.0] [G loss: 0.0004707247135229409]\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "16 [D loss: 0.0007355942943831906, acc.: 100.0] [G loss: 0.0004119229270145297]\n",
      "1/1 [==============================] - 0s 300ms/step\n",
      "17 [D loss: 0.0007311090303119272, acc.: 100.0] [G loss: 0.00031679694075137377]\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "18 [D loss: 0.0006138984026620165, acc.: 100.0] [G loss: 0.0002873883640859276]\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "19 [D loss: 0.0005580725555773824, acc.: 100.0] [G loss: 0.0002509862242732197]\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "20 [D loss: 0.0004938093334203586, acc.: 100.0] [G loss: 0.0002220946189481765]\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "21 [D loss: 0.0005138072592671961, acc.: 100.0] [G loss: 0.00020849835709668696]\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "22 [D loss: 0.00048540427815169096, acc.: 100.0] [G loss: 0.00017199583817273378]\n",
      "1/1 [==============================] - 0s 407ms/step\n",
      "23 [D loss: 0.0004644538275897503, acc.: 100.0] [G loss: 0.00016340153524652123]\n",
      "1/1 [==============================] - 0s 422ms/step\n",
      "24 [D loss: 0.0005130446224939078, acc.: 100.0] [G loss: 0.00013958194176666439]\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "25 [D loss: 0.0006585704395547509, acc.: 100.0] [G loss: 0.00012352110934443772]\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "26 [D loss: 0.0013720120477955788, acc.: 100.0] [G loss: 0.00012123537453589961]\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "27 [D loss: 0.003198240287019871, acc.: 100.0] [G loss: 0.00013291926006786525]\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "28 [D loss: 0.00658316895714961, acc.: 100.0] [G loss: 0.00018378863751422614]\n",
      "1/1 [==============================] - 0s 262ms/step\n",
      "29 [D loss: 0.007725565563305281, acc.: 100.0] [G loss: 0.0005403832183219492]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "30 [D loss: 0.006555690779350698, acc.: 100.0] [G loss: 0.003868845524266362]\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "31 [D loss: 0.008459949865937233, acc.: 100.0] [G loss: 0.05797895789146423]\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "32 [D loss: 2.7305255425162613, acc.: 50.0] [G loss: 6.7482099533081055]\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "33 [D loss: 1.9265189170837402, acc.: 48.4375] [G loss: 0.38866445422172546]\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "34 [D loss: 0.1709094885736704, acc.: 100.0] [G loss: 0.19665174186229706]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "35 [D loss: 0.08749947231262922, acc.: 100.0] [G loss: 0.1351584643125534]\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "36 [D loss: 0.12945351749658585, acc.: 100.0] [G loss: 0.09578145295381546]\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "37 [D loss: 0.42826639115810394, acc.: 65.625] [G loss: 0.15381184220314026]\n",
      "1/1 [==============================] - 0s 390ms/step\n",
      "38 [D loss: 0.5854698233306408, acc.: 50.0] [G loss: 0.5572590231895447]\n",
      "1/1 [==============================] - 0s 354ms/step\n",
      "39 [D loss: 0.24038243293762207, acc.: 100.0] [G loss: 0.8891252279281616]\n",
      "1/1 [==============================] - 0s 329ms/step\n",
      "40 [D loss: 0.193304686807096, acc.: 96.875] [G loss: 0.3690710663795471]\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "41 [D loss: 0.13406232930719852, acc.: 100.0] [G loss: 0.13284438848495483]\n",
      "1/1 [==============================] - 0s 308ms/step\n",
      "42 [D loss: 0.15135018527507782, acc.: 100.0] [G loss: 0.14060238003730774]\n",
      "1/1 [==============================] - 0s 325ms/step\n",
      "43 [D loss: 0.1420818343758583, acc.: 100.0] [G loss: 0.1942950189113617]\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "44 [D loss: 1.1111890375614166, acc.: 48.4375] [G loss: 2.4292263984680176]\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "45 [D loss: 0.8334278017282486, acc.: 50.0] [G loss: 1.1055405139923096]\n",
      "1/1 [==============================] - 0s 353ms/step\n",
      "46 [D loss: 0.24630022048950195, acc.: 96.875] [G loss: 0.9023385047912598]\n",
      "1/1 [==============================] - 0s 391ms/step\n",
      "47 [D loss: 0.16143420338630676, acc.: 100.0] [G loss: 1.0802844762802124]\n",
      "1/1 [==============================] - 0s 326ms/step\n",
      "48 [D loss: 0.22622652351856232, acc.: 100.0] [G loss: 1.4083789587020874]\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "49 [D loss: 0.1090908357873559, acc.: 100.0] [G loss: 0.923423171043396]\n",
      "1/1 [==============================] - 0s 373ms/step\n",
      "50 [D loss: 0.10183185117784888, acc.: 100.0] [G loss: 0.3784921169281006]\n",
      "1/1 [==============================] - 0s 413ms/step\n",
      "51 [D loss: 0.05654932139441371, acc.: 100.0] [G loss: 0.17060962319374084]\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "52 [D loss: 0.07530267164111137, acc.: 100.0] [G loss: 0.17859522998332977]\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "53 [D loss: 0.09953609481453896, acc.: 100.0] [G loss: 0.3672007620334625]\n",
      "1/1 [==============================] - 0s 372ms/step\n",
      "54 [D loss: 0.20900484174489975, acc.: 100.0] [G loss: 2.119147777557373]\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "55 [D loss: 0.38853342831134796, acc.: 79.6875] [G loss: 0.6551573276519775]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "56 [D loss: 0.08516813442111015, acc.: 100.0] [G loss: 0.3668650686740875]\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "57 [D loss: 0.06796596571803093, acc.: 100.0] [G loss: 0.25360551476478577]\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "58 [D loss: 0.060972291976213455, acc.: 100.0] [G loss: 0.326498806476593]\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "59 [D loss: 0.17756742984056473, acc.: 100.0] [G loss: 2.0601797103881836]\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "60 [D loss: 0.17196623235940933, acc.: 100.0] [G loss: 3.010798692703247]\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "61 [D loss: 0.23946573585271835, acc.: 92.1875] [G loss: 0.6453036665916443]\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "62 [D loss: 0.21537819504737854, acc.: 98.4375] [G loss: 2.5831265449523926]\n",
      "1/1 [==============================] - 0s 262ms/step\n",
      "63 [D loss: 0.20725683867931366, acc.: 96.875] [G loss: 3.6743528842926025]\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "64 [D loss: 0.2492009475827217, acc.: 90.625] [G loss: 0.6067718863487244]\n",
      "1/1 [==============================] - 0s 297ms/step\n",
      "65 [D loss: 0.040420531295239925, acc.: 100.0] [G loss: 0.22884799540042877]\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "66 [D loss: 0.057678401470184326, acc.: 100.0] [G loss: 0.36230239272117615]\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "67 [D loss: 0.07517686672508717, acc.: 100.0] [G loss: 1.0510401725769043]\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "68 [D loss: 0.159773088991642, acc.: 100.0] [G loss: 3.5384302139282227]\n",
      "1/1 [==============================] - 0s 423ms/step\n",
      "69 [D loss: 0.5736982822418213, acc.: 85.9375] [G loss: 4.163434982299805]\n",
      "1/1 [==============================] - 0s 298ms/step\n",
      "70 [D loss: 0.328299380838871, acc.: 90.625] [G loss: 2.1341681480407715]\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "71 [D loss: 0.2877863049507141, acc.: 89.0625] [G loss: 1.9812045097351074]\n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "72 [D loss: 0.40206728875637054, acc.: 85.9375] [G loss: 1.0755400657653809]\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "73 [D loss: 0.39694570004940033, acc.: 89.0625] [G loss: 1.0672634840011597]\n",
      "1/1 [==============================] - 0s 411ms/step\n",
      "74 [D loss: 0.17650749161839485, acc.: 96.875] [G loss: 0.4053511619567871]\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "75 [D loss: 0.2707486227154732, acc.: 93.75] [G loss: 0.5563111305236816]\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "76 [D loss: 0.4284740835428238, acc.: 78.125] [G loss: 1.3087599277496338]\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "77 [D loss: 0.7374122142791748, acc.: 46.875] [G loss: 1.3083438873291016]\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "78 [D loss: 0.4595862925052643, acc.: 89.0625] [G loss: 2.000248908996582]\n",
      "1/1 [==============================] - 0s 326ms/step\n",
      "79 [D loss: 0.38027018308639526, acc.: 96.875] [G loss: 1.2800519466400146]\n",
      "1/1 [==============================] - 0s 297ms/step\n",
      "80 [D loss: 0.35227925330400467, acc.: 92.1875] [G loss: 2.2656736373901367]\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "81 [D loss: 0.4194563329219818, acc.: 92.1875] [G loss: 2.376040458679199]\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "82 [D loss: 0.5590580999851227, acc.: 82.8125] [G loss: 2.495784044265747]\n",
      "1/1 [==============================] - 1s 541ms/step\n",
      "83 [D loss: 0.5995833873748779, acc.: 71.875] [G loss: 1.9747488498687744]\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "84 [D loss: 0.34565481543540955, acc.: 90.625] [G loss: 1.6741642951965332]\n",
      "1/1 [==============================] - 0s 298ms/step\n",
      "85 [D loss: 0.8646973967552185, acc.: 60.9375] [G loss: 3.316110849380493]\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "86 [D loss: 1.7528966069221497, acc.: 4.6875] [G loss: 2.681607723236084]\n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "87 [D loss: 0.5417963117361069, acc.: 68.75] [G loss: 2.377458333969116]\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "88 [D loss: 0.5792929530143738, acc.: 73.4375] [G loss: 2.7371883392333984]\n",
      "1/1 [==============================] - 0s 365ms/step\n",
      "89 [D loss: 0.3904724791646004, acc.: 90.625] [G loss: 1.1961095333099365]\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "90 [D loss: 0.44371896982192993, acc.: 81.25] [G loss: 0.7850898504257202]\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "91 [D loss: 0.5368019640445709, acc.: 70.3125] [G loss: 1.357811689376831]\n",
      "1/1 [==============================] - 0s 319ms/step\n",
      "92 [D loss: 0.3958136737346649, acc.: 95.3125] [G loss: 1.2740877866744995]\n",
      "1/1 [==============================] - 0s 327ms/step\n",
      "93 [D loss: 0.7094584107398987, acc.: 50.0] [G loss: 1.667837381362915]\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "94 [D loss: 0.48934532701969147, acc.: 89.0625] [G loss: 1.8566733598709106]\n",
      "1/1 [==============================] - 0s 378ms/step\n",
      "95 [D loss: 0.6604689955711365, acc.: 65.625] [G loss: 1.5632833242416382]\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "96 [D loss: 0.44311489164829254, acc.: 87.5] [G loss: 1.8170037269592285]\n",
      "1/1 [==============================] - 0s 319ms/step\n",
      "97 [D loss: 0.5475732684135437, acc.: 76.5625] [G loss: 1.528853416442871]\n",
      "1/1 [==============================] - 0s 378ms/step\n",
      "98 [D loss: 0.5744788646697998, acc.: 65.625] [G loss: 1.3834221363067627]\n",
      "1/1 [==============================] - 0s 369ms/step\n",
      "99 [D loss: 0.541428804397583, acc.: 78.125] [G loss: 1.360861897468567]\n",
      "1/1 [==============================] - 0s 136ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "# Create a nested folder for the generated images\n",
    "output_folder = './Data/fits_filtered2/augmented/gan_output4'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Step 2: Load and Preprocess Dataset\n",
    "def load_images_from_folder(folder, image_size=(64, 64)):\n",
    "    images = []\n",
    "    valid_files = 0\n",
    "    invalid_files = 0\n",
    "    for filename in os.listdir(folder):\n",
    "        try:\n",
    "            img = load_img(os.path.join(folder, filename), target_size=image_size)\n",
    "            if img is not None:\n",
    "                images.append(img_to_array(img))\n",
    "                valid_files += 1\n",
    "            else:\n",
    "                invalid_files += 1\n",
    "        except (UnidentifiedImageError, OSError):\n",
    "            print(f\"Skipping file {filename}, as it is not a valid image.\")\n",
    "            invalid_files += 1\n",
    "    print(f\"Loaded {valid_files} valid images, skipped {invalid_files} invalid images.\")\n",
    "    return np.array(images)\n",
    "\n",
    "dataset = load_images_from_folder('./Data/fits_filtered2/augmented')\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "if dataset.size == 0:\n",
    "    raise ValueError(\"No valid images found in the dataset. Please check the image files.\")\n",
    "dataset = (dataset - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "\n",
    "# Step 3: Build the GAN\n",
    "\n",
    "# Generator\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256 * 16 * 16, activation=\"relu\", input_dim=100))\n",
    "    model.add(layers.Reshape((16, 16, 256)))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.UpSampling2D())\n",
    "    model.add(layers.Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "    model.add(layers.UpSampling2D())\n",
    "    model.add(layers.Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "    model.add(layers.Conv2D(3, kernel_size=4, padding=\"same\"))\n",
    "    model.add(layers.Activation(\"tanh\"))\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, kernel_size=4, strides=2, input_shape=(64, 64, 3), padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))  # New layer with more filters\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# The generator takes noise as input and generates images\n",
    "z = layers.Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model, only the generator is trained\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model (stacked generator and discriminator)\n",
    "combined = tf.keras.Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))\n",
    "\n",
    "# Step 4: Train the GAN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "save_interval = 1000\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "X_train = dataset\n",
    "half_batch = int(batch_size / 2)\n",
    "\n",
    "# Training the GAN\n",
    "for epoch in range(epochs):\n",
    "    # Train Discriminator\n",
    "    idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "    imgs = X_train[idx]\n",
    "\n",
    "    noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train Generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    valid_y = np.array([1] * batch_size)\n",
    "\n",
    "    g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "    print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
    "\n",
    "    # If at save interval => save generated image samples\n",
    "    if epoch % save_interval == 0:\n",
    "        noise = np.random.normal(0, 1, (25, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5  # Rescale images 0 - 1\n",
    "\n",
    "        fig, axs = plt.subplots(5, 5)\n",
    "        cnt = 0\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                axs[i, j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.savefig(os.path.join(output_folder, f'epoch_{epoch}.png'))  # Save the figure\n",
    "        plt.close()  # Close the figure to free up memory\n",
    "\n",
    "# Step 5: Generate New Data\n",
    "\n",
    "noise = np.random.normal(0, 1, (10, 100))\n",
    "gen_imgs = generator.predict(noise)\n",
    "gen_imgs = 0.5 * gen_imgs + 0.5  # Rescale images 0 - 1\n",
    "\n",
    "for i in range(10):\n",
    "    plt.imshow(gen_imgs[i])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(output_folder, f'final_{i}.png'))  # Save the figure\n",
    "    plt.close()  # Close the figure to free up memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
