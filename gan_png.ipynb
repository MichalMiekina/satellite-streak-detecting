{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Skipping file gan_output, as it is not a valid image.\n",
      "Skipping file gan_output2, as it is not a valid image.\n",
      "Skipping file gan_output3, as it is not a valid image.\n",
      "Loaded 32 valid images, skipped 3 invalid images.\n",
      "Dataset shape: (32, 128, 128, 3)\n",
      "WARNING:tensorflow:From c:\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python310\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "WARNING:tensorflow:From c:\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "0 [D loss: 0.12245893478393555, acc.: 34.375] [G loss: 0.5129153728485107]\n",
      "1/1 [==============================] - 0s 390ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1 [D loss: -0.13370664790272713, acc.: 1.5625] [G loss: 0.5676045417785645]\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "2 [D loss: -0.30531225353479385, acc.: 0.0] [G loss: 0.6513931751251221]\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "3 [D loss: -0.3629514202475548, acc.: 3.125] [G loss: 0.7515891790390015]\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "4 [D loss: -0.40273595601320267, acc.: 7.8125] [G loss: 0.8006399869918823]\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "5 [D loss: -0.45866600796580315, acc.: 0.0] [G loss: 0.8259836435317993]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "6 [D loss: -0.4490377977490425, acc.: 3.125] [G loss: 0.8268942832946777]\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "7 [D loss: -0.4685666561126709, acc.: 1.5625] [G loss: 0.8270992040634155]\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "8 [D loss: -0.46246751211583614, acc.: 1.5625] [G loss: 0.8375570774078369]\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "9 [D loss: -0.46797536686062813, acc.: 1.5625] [G loss: 0.8110390305519104]\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "10 [D loss: -0.4874419146217406, acc.: 0.0] [G loss: 0.7698819041252136]\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "11 [D loss: -0.47360123693943024, acc.: 1.5625] [G loss: 0.7645846009254456]\n",
      "1/1 [==============================] - 0s 304ms/step\n",
      "12 [D loss: -0.4844287922605872, acc.: 1.5625] [G loss: 0.6495926380157471]\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "13 [D loss: -0.48686380963772535, acc.: 0.0] [G loss: 0.5554502606391907]\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "14 [D loss: -0.47316783107817173, acc.: 0.0] [G loss: 0.5074983835220337]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "15 [D loss: -0.4920763608533889, acc.: 0.0] [G loss: 0.38520029187202454]\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "16 [D loss: -0.48351984564214945, acc.: 0.0] [G loss: 0.2980359196662903]\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "17 [D loss: -0.4909397822339088, acc.: 0.0] [G loss: 0.16679954528808594]\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "18 [D loss: -0.4881429187953472, acc.: 0.0] [G loss: 0.12977153062820435]\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "19 [D loss: -0.4837809740565717, acc.: 0.0] [G loss: 0.15972676873207092]\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "20 [D loss: -0.48092460446059704, acc.: 0.0] [G loss: 0.10417596995830536]\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "21 [D loss: -0.4798722204286605, acc.: 0.0] [G loss: 0.13941556215286255]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "22 [D loss: -0.4711232357658446, acc.: 0.0] [G loss: 0.21243230998516083]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "23 [D loss: -0.4797543063759804, acc.: 0.0] [G loss: 0.38443857431411743]\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "24 [D loss: -0.4684797441586852, acc.: 0.0] [G loss: 0.5300554037094116]\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "25 [D loss: -0.4812087770551443, acc.: 0.0] [G loss: 0.7291480302810669]\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "26 [D loss: -0.46011294797062874, acc.: 0.0] [G loss: 0.5935221314430237]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "27 [D loss: -0.4749338454566896, acc.: 0.0] [G loss: 0.7384441494941711]\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "28 [D loss: -0.47613635985180736, acc.: 0.0] [G loss: 0.8973751664161682]\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "29 [D loss: -0.48174041602760553, acc.: 0.0] [G loss: 0.9542323350906372]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "30 [D loss: -0.4689426217228174, acc.: 0.0] [G loss: 0.8998894095420837]\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "31 [D loss: -0.4693667106330395, acc.: 0.0] [G loss: 0.9268380999565125]\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "32 [D loss: -0.47570712957531214, acc.: 0.0] [G loss: 0.9868310689926147]\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "33 [D loss: -0.4834687886759639, acc.: 0.0] [G loss: 0.9951456785202026]\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "34 [D loss: -0.4697951413691044, acc.: 0.0] [G loss: 0.9918802976608276]\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "35 [D loss: -0.48862913937773556, acc.: 0.0] [G loss: 0.9960963129997253]\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "36 [D loss: -0.48944053146988153, acc.: 0.0] [G loss: 0.9936460852622986]\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "37 [D loss: -0.49072404857724905, acc.: 0.0] [G loss: 0.9947280883789062]\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "38 [D loss: -0.49030107958242297, acc.: 0.0] [G loss: 0.9932689070701599]\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "39 [D loss: -0.4906064241658896, acc.: 0.0] [G loss: 0.9954559206962585]\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "40 [D loss: -0.49232029961422086, acc.: 0.0] [G loss: 0.9962810277938843]\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "41 [D loss: -0.48975702468305826, acc.: 0.0] [G loss: 0.9962383508682251]\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "42 [D loss: -0.4936410062946379, acc.: 0.0] [G loss: 0.9967914819717407]\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "43 [D loss: -0.4955759986769408, acc.: 0.0] [G loss: 0.9983652830123901]\n",
      "1/1 [==============================] - 0s 328ms/step\n",
      "44 [D loss: -0.48710154835134745, acc.: 0.0] [G loss: 0.9962229132652283]\n",
      "1/1 [==============================] - 0s 373ms/step\n",
      "45 [D loss: -0.4943641407880932, acc.: 0.0] [G loss: 0.9968297481536865]\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "46 [D loss: -0.49534308549482375, acc.: 0.0] [G loss: 0.9976611137390137]\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "47 [D loss: -0.49613883974961936, acc.: 0.0] [G loss: 0.9979621171951294]\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "48 [D loss: -0.49585410789586604, acc.: 0.0] [G loss: 0.9981666207313538]\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "49 [D loss: -0.49556743865832686, acc.: 0.0] [G loss: 0.9983381032943726]\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "50 [D loss: -0.4953370450530201, acc.: 0.0] [G loss: 0.9990823268890381]\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "51 [D loss: -0.49753214250085875, acc.: 0.0] [G loss: 0.9990056753158569]\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "52 [D loss: -0.49635649425908923, acc.: 0.0] [G loss: 0.9991889595985413]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "53 [D loss: -0.4971534644719213, acc.: 0.0] [G loss: 0.9991493225097656]\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "54 [D loss: -0.4936504797078669, acc.: 0.0] [G loss: 0.9988430738449097]\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "55 [D loss: -0.4979205271229148, acc.: 0.0] [G loss: 0.999147891998291]\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "56 [D loss: -0.4971901949029416, acc.: 0.0] [G loss: 0.9991997480392456]\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "57 [D loss: -0.49775723181664944, acc.: 0.0] [G loss: 0.9992835521697998]\n",
      "1/1 [==============================] - 0s 288ms/step\n",
      "58 [D loss: -0.496786015573889, acc.: 0.0] [G loss: 0.9985793232917786]\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "59 [D loss: -0.49804195819888264, acc.: 0.0] [G loss: 0.998780369758606]\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "60 [D loss: -0.4968306148657575, acc.: 0.0] [G loss: 0.998737633228302]\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "61 [D loss: -0.4962966125458479, acc.: 0.0] [G loss: 0.9993147850036621]\n",
      "1/1 [==============================] - 0s 351ms/step\n",
      "62 [D loss: -0.49887177022174, acc.: 0.0] [G loss: 0.9990730285644531]\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "63 [D loss: -0.4976056874729693, acc.: 0.0] [G loss: 0.9986720681190491]\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "64 [D loss: -0.4986879623029381, acc.: 0.0] [G loss: 0.9991198182106018]\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "65 [D loss: -0.49868635524762794, acc.: 0.0] [G loss: 0.9989659190177917]\n",
      "1/1 [==============================] - 0s 447ms/step\n",
      "66 [D loss: -0.4988805496250279, acc.: 0.0] [G loss: 0.9992104768753052]\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "67 [D loss: -0.49819366564042866, acc.: 0.0] [G loss: 0.9988979697227478]\n",
      "1/1 [==============================] - 0s 467ms/step\n",
      "68 [D loss: -0.49758433643728495, acc.: 0.0] [G loss: 0.9988042116165161]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "69 [D loss: -0.49945053717237897, acc.: 0.0] [G loss: 0.9988924264907837]\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "70 [D loss: -0.49731551529839635, acc.: 0.0] [G loss: 0.9989503622055054]\n",
      "1/1 [==============================] - 0s 290ms/step\n",
      "71 [D loss: -0.4980774435098283, acc.: 0.0] [G loss: 0.9992235898971558]\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "72 [D loss: -0.49889357591746375, acc.: 0.0] [G loss: 0.999250054359436]\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "73 [D loss: -0.4984229624969885, acc.: 0.0] [G loss: 0.999402642250061]\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "74 [D loss: -0.49919386848341674, acc.: 0.0] [G loss: 0.9991851449012756]\n",
      "1/1 [==============================] - 0s 377ms/step\n",
      "75 [D loss: -0.49900383557542227, acc.: 0.0] [G loss: 0.9994422197341919]\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "76 [D loss: -0.4991140667843865, acc.: 0.0] [G loss: 0.9994109869003296]\n",
      "1/1 [==============================] - 0s 390ms/step\n",
      "77 [D loss: -0.4991807861952111, acc.: 0.0] [G loss: 0.9995488524436951]\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "78 [D loss: -0.49860180146060884, acc.: 0.0] [G loss: 0.9996077418327332]\n",
      "1/1 [==============================] - 0s 407ms/step\n",
      "79 [D loss: -0.49849353171885014, acc.: 0.0] [G loss: 0.9989964962005615]\n",
      "1/1 [==============================] - 0s 365ms/step\n",
      "80 [D loss: -0.49894596182275563, acc.: 0.0] [G loss: 0.9996526837348938]\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "81 [D loss: -0.49932727485429496, acc.: 0.0] [G loss: 0.9994837045669556]\n",
      "1/1 [==============================] - 0s 315ms/step\n",
      "82 [D loss: -0.49944552467786707, acc.: 0.0] [G loss: 0.9994024038314819]\n",
      "1/1 [==============================] - 0s 355ms/step\n",
      "83 [D loss: -0.499310643208446, acc.: 0.0] [G loss: 0.9995505809783936]\n",
      "1/1 [==============================] - 0s 289ms/step\n",
      "84 [D loss: -0.4994847213383764, acc.: 0.0] [G loss: 0.9995533227920532]\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "85 [D loss: -0.4992821675550658, acc.: 0.0] [G loss: 0.999485433101654]\n",
      "1/1 [==============================] - 0s 366ms/step\n",
      "86 [D loss: -0.4981577650178224, acc.: 0.0] [G loss: 0.9994735717773438]\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "87 [D loss: -0.4986652196967043, acc.: 0.0] [G loss: 0.999383270740509]\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "88 [D loss: -0.4991964110522531, acc.: 0.0] [G loss: 0.9992627501487732]\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "89 [D loss: -0.4991853178071324, acc.: 0.0] [G loss: 0.9993956685066223]\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "90 [D loss: -0.49865467892959714, acc.: 0.0] [G loss: 0.9993585348129272]\n",
      "1/1 [==============================] - 0s 372ms/step\n",
      "91 [D loss: -0.4994652247405611, acc.: 0.0] [G loss: 0.9993857145309448]\n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "92 [D loss: -0.49946908657148015, acc.: 0.0] [G loss: 0.9991697072982788]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "93 [D loss: -0.4994143024669029, acc.: 0.0] [G loss: 0.9994874596595764]\n",
      "1/1 [==============================] - 0s 492ms/step\n",
      "94 [D loss: -0.4990487861796282, acc.: 0.0] [G loss: 0.9993455410003662]\n",
      "1/1 [==============================] - 0s 302ms/step\n",
      "95 [D loss: -0.4993443350249436, acc.: 0.0] [G loss: 0.9996304512023926]\n",
      "1/1 [==============================] - 0s 347ms/step\n",
      "96 [D loss: -0.49963801410922315, acc.: 0.0] [G loss: 0.9992433786392212]\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "97 [D loss: -0.4995003828771587, acc.: 0.0] [G loss: 0.999618649482727]\n",
      "1/1 [==============================] - 0s 367ms/step\n",
      "98 [D loss: -0.499161579326028, acc.: 0.0] [G loss: 0.9996440410614014]\n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "99 [D loss: -0.49868330982280895, acc.: 0.0] [G loss: 0.999605119228363]\n",
      "1/1 [==============================] - 0s 132ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "from PIL import UnidentifiedImageError\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "# Create a nested folder for the generated images\n",
    "output_folder = './Data/fits_filtered9/augmented_images/gan_output2'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Step 1: Load and Preprocess Dataset\n",
    "def load_images_from_folder(folder, image_size=(128, 128)):\n",
    "    images = []\n",
    "    valid_files = 0\n",
    "    invalid_files = 0\n",
    "    for filename in os.listdir(folder):\n",
    "        try:\n",
    "            img = load_img(os.path.join(folder, filename), target_size=image_size)\n",
    "            if img is not None:\n",
    "                images.append(img_to_array(img))\n",
    "                valid_files += 1\n",
    "            else:\n",
    "                invalid_files += 1\n",
    "        except (UnidentifiedImageError, OSError):\n",
    "            print(f\"Skipping file {filename}, as it is not a valid image.\")\n",
    "            invalid_files += 1\n",
    "    print(f\"Loaded {valid_files} valid images, skipped {invalid_files} invalid images.\")\n",
    "    return np.array(images)\n",
    "\n",
    "dataset = load_images_from_folder('./Data/fits_filtered9/augmented_images')\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "if dataset.size == 0:\n",
    "    raise ValueError(\"No valid images found in the dataset. Please check the image files.\")\n",
    "dataset = (dataset - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "\n",
    "# Step 2: Build the GAN\n",
    "\n",
    "# Generator\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(512 * 8 * 8, activation=\"relu\", input_dim=100))\n",
    "    model.add(layers.Reshape((8, 8, 512)))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "\n",
    "    model.add(layers.UpSampling2D())  # 16x16\n",
    "    model.add(layers.Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "\n",
    "    model.add(layers.UpSampling2D())  # 32x32\n",
    "    model.add(layers.Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "\n",
    "    model.add(layers.UpSampling2D())  # 64x64\n",
    "    model.add(layers.Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "\n",
    "    model.add(layers.UpSampling2D())  # 128x128\n",
    "    model.add(layers.Conv2D(3, kernel_size=3, padding=\"same\"))\n",
    "    model.add(layers.Activation(\"tanh\"))\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, input_shape=(128, 128, 3), padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(512, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "# Perceptual loss\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    vgg = VGG19(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "    vgg.trainable = False\n",
    "    model = tf.keras.Model(inputs=vgg.input, outputs=vgg.get_layer('block5_conv4').output)\n",
    "    y_true_features = model(y_true)\n",
    "    y_pred_features = model(y_pred)\n",
    "    return K.mean(K.square(y_true_features - y_pred_features))\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss=wasserstein_loss, optimizer=tf.keras.optimizers.Adam(0.0001, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# The generator takes noise as input and generates images\n",
    "z = layers.Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model, only the generator is trained\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model (stacked generator and discriminator)\n",
    "combined = tf.keras.Model(z, valid)\n",
    "combined.compile(loss=[wasserstein_loss], optimizer=tf.keras.optimizers.Adam(0.0001, 0.5))\n",
    "\n",
    "# Step 3: Train the GAN\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "save_interval = 500\n",
    "\n",
    "X_train = dataset\n",
    "half_batch = int(batch_size / 2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train Discriminator\n",
    "    idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "    imgs = X_train[idx]\n",
    "\n",
    "    noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, -np.ones((half_batch, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train Generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    valid_y = np.ones((batch_size, 1))\n",
    "\n",
    "    g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "    print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
    "\n",
    "    if epoch % save_interval == 0:\n",
    "        noise = np.random.normal(0, 1, (25, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5  # Rescale images 0 - 1\n",
    "\n",
    "        fig, axs = plt.subplots(5, 5)\n",
    "        cnt = 0\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                axs[i, j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.savefig(os.path.join(output_folder, f'epoch_{epoch}.png'))\n",
    "        plt.close()\n",
    "\n",
    "# Generate final samples\n",
    "noise = np.random.normal(0, 1, (10, 100))\n",
    "gen_imgs = generator.predict(noise)\n",
    "gen_imgs = 0.5 * gen_imgs + 0.5  # Rescale images 0 - 1\n",
    "\n",
    "for i in range(10):\n",
    "    plt.imshow(gen_imgs[i])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(output_folder, f'final_{i}.png'))\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
